## NLP (Natural Language Processing)
**Objective**: This repository is created to capture pointers, guidance for fundamentals around NLP (Natural Language Processing) from learning perspective, innovation / research areas etc. It also throws light into recommended subject areas, content relating to accelerating in the journey of learning in this field.

**Target Audience**: Data Science and AI Practitioners with already having fundamental, working knowledge and familiarity of Machine Learning concepts, Python/R/SQL programming background.

**Areas of Focus:**
- [Research Focus and trends](https://github.com/kkm24132/DataScience_NLP/blob/main/README.md#research-focus-and-trends)
- [Intro and Learning Content](https://github.com/kkm24132/DataScience_NLP/blob/main/README.md#intro-and-learning-content)
- [Techniques](https://github.com/kkm24132/DataScience_NLP/blob/main/README.md#techniques)
- Libraries / packages
- Services
- Datasets
- Video and Online Content references




Area           |Description                                     |  Target Timeline |
:--            |:--                                             |        --        |
Pre-Requisites |<ul> <li>Familiarity with Python Programming - [Some Ref](https://github.com/kkm24132/Mentoring_Enablement/tree/master/Python)</li> <li> [Descriptive Stats](https://www.khanacademy.org/math/engageny-alg-1/alg1-2) by Khan Academy </li> <li> The Elements of Statistical Learning - [ISLR Book Reference by Hasti,Tishirani et al](https://web.stanford.edu/~hastie/Papers/ESLII.pdf)</li> <li> Machine Learning Fundamentals [Andrew Ng's course around ML](https://www.coursera.org/learn/machine-learning) </li> <li> Familiarity with Data Science processes and frameworks [CRISP-DM](https://en.wikipedia.org/wiki/Cross-industry_standard_process_for_data_mining) </li></ul> | Week 0
Handling Text Processing |<ul> <li>Text pre-processing techniques (Familiarity with [spaCy](https://spacy.io/usage) library, familiarity with [NLTK](https://www.nltk.org/) library, [Tokenization using spaCy library](https://medium.com/@makcedward/nlp-pipeline-word-tokenization-part-1-4b2b547e6a3), [Stopword removal and text normalization](https://www.analyticsvidhya.com/blog/2019/08/how-to-remove-stopwords-text-normalization-nltk-spacy-gensim-python/?utm_source=blog&utm_medium=learning-path-nlp-2020) )</li> <li> Regular expressions </li> <li> [Exploratory Analysis](https://towardsdatascience.com/a-complete-exploratory-data-analysis-and-visualization-for-text-data-29fb1b96fb6a) with Text data </li>  <li> [Extract Meta Features from text](https://towardsdatascience.com/understanding-feature-engineering-part-3-traditional-methods-for-text-data-f6f7d70acd41) </li> <li> Build a text classification model [Practice Problem - Identify Sentiments](https://datahack.analyticsvidhya.com/contest/linguipedia-codefest-natural-language-processing-1/?utm_source=blog&utm_medium=learning-path-nlp-2020#LeaderBoard) ..can be any such equivalent problem for experience </li></ul> | Week 1-4

## References

- [Deep Learning for NLP : without Magic](https://www.socher.org/index.php/DeepLearningTutorial/DeepLearningTutorial)
- [Stanford NLP](https://nlp.stanford.edu/teaching/)
- [BERT, ELMo and GPT2](http://ai.stanford.edu/blog/contextual/) How contextual are Contexualized Word Representations? - from Stanford AI Lab
- [The Illustrated BERT, ELMo and others](http://jalammar.github.io/illustrated-bert/) NLP and transfer learning context

## Research Focus and Trends

## Intro and Learning Content

## Techniques

## Libraries / Packages

## Services

## Datasets

## Video and Online Content References

## Research Papers

- [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://openreview.net/forum?id=H1eA7AEtvS), Related [Code](https://github.com/google-research/ALBERT)
- [A Mutual Information Maximization Perspective of Language Representation Learning](https://openreview.net/forum?id=Syx79eBKwr)
- [DeFINE: Deep Factorized Input Token Embeddings for Neural Sequence Modeling](https://openreview.net/forum?id=rJeXS04FPH)



``` 
Disclaimer: Information represented here is based on my own experiences, learnings, readings and no way represent any firm's opinion, strategy etc or any individual's opinion or not intended for anything else other than learning and/or research/innovation in the field. Content here and on this repository is non-exhaustive and continuous improvement / continuous learning focus is needed to learn more. 

Recommendation - Keep Learning and keep improving.
```
