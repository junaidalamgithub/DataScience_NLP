## NLP (Natural Language Processing)
**Objective**: This repository is created to capture pointers, guidance for fundamentals around NLP (Natural Language Processing) from learning perspective, innovation / research areas etc. It also throws light into recommended subject areas, content relating to accelerating in the journey of learning in this field.

**Target Audience**: Data Science and AI Practitioners with already having fundamental, working knowledge and familiarity of Machine Learning concepts, Python/R/SQL programming background.

**Areas of Focus:**
- [Research Focus and trends](https://github.com/kkm24132/DataScience_NLP/blob/main/README.md#research-focus-and-trends)
- [Intro and Learning Content](https://github.com/kkm24132/DataScience_NLP/blob/main/README.md#intro-and-learning-content)
- [Techniques](https://github.com/kkm24132/DataScience_NLP/blob/main/README.md#techniques)
- [Libraries / packages](https://github.com/kkm24132/DataScience_NLP/blob/main/README.md#libraries--packages)
- [Services](https://github.com/kkm24132/DataScience_NLP/blob/main/README.md#services)
- [Datasets](https://github.com/kkm24132/DataScience_NLP/blob/main/README.md#datasets)
- Video and Online Content references


## Research Focus and Trends
- Please keep referring to NLP related research papers from AAAI, NeurIPS, ACL, ICLR and similar conferences for latest research focus areas. Most of these may be captured in the arXiv.org site as well.
- Few latest and key research papers for reading are as follows: (Please note this keeps changing and may not be dated)
  - [WinoGrande: An Adversarial Winograd Schema Challenge at Scale](https://arxiv.org/abs/1907.10641) - the [GitHub](https://github.com/allenai/winogrande) page
  - [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683) - the [GitHub](https://github.com/google-research/text-to-text-transfer-transformer) page with pretrained models along with the dataset and code
  - [Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451) - the [GitHub page with official code implementation from Google](https://github.com/google/trax/tree/master/trax/models/reformer) and the [GitHub page with PyTorch implementation of Reformer](https://github.com/lucidrains/reformer-pytorch)
  - [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150) - the [GitHub page](https://github.com/allenai/longformer)
- [NLP-Progress](https://nlpprogress.com/) tracks the progress in Natural Language Processing, including the datasets and the current state-of-the-art for the most common NLP tasks.
- [NLP-Overview](https://nlpoverview.com/) is an up-to-date overview of deep learning techniques applied to NLP, including theory, implementations, applications, and state-of-the-art results. This is a great Deep NLP Introduction for researchers.
- [NLP's ImageNet moment](https://thegradient.pub/nlp-imagenet/)
- [ACL 2018 Highlights: Understanding Representations and Evaluation in More Challenging Settings](https://ruder.io/acl-2018-highlights/)
- [Four deep learning trends from ACL 2017 - Part 1](https://www.abigailsee.com/2017/08/30/four-deep-learning-trends-from-acl-2017-part-1.html) - Linguistic Structure and Word Embeddings
- [Four deep learning trends from ACL 2017 - Part 2](https://www.abigailsee.com/2017/08/30/four-deep-learning-trends-from-acl-2017-part-2.html) - Interpretability and Attention
- [Deep Learning for NLP: Advancements & Trends](https://tryolabs.com/blog/2017/12/12/deep-learning-for-nlp-advancements-and-trends-in-2017/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=The%20Wild%20Week%20in%20AI)
- [Deep Learning for NLP : without Magic](https://www.socher.org/index.php/DeepLearningTutorial/DeepLearningTutorial)
- [Stanford NLP](https://nlp.stanford.edu/teaching/)
- [BERT, ELMo and GPT2](http://ai.stanford.edu/blog/contextual/) How contextual are Contexualized Word Representations? - from Stanford AI Lab
- [The Illustrated BERT, ELMo and others](http://jalammar.github.io/illustrated-bert/) NLP and transfer learning context


## Intro and Learning Content

Area           |Description                                     |  Target Timeline |
:--            |:--                                             |        --        |
Pre-Requisites |<ul> <li>Familiarity with Python Programming - [Some Ref](https://github.com/kkm24132/Mentoring_Enablement/tree/master/Python)</li> <li> [Descriptive Stats](https://www.khanacademy.org/math/engageny-alg-1/alg1-2) by Khan Academy </li> <li> The Elements of Statistical Learning - [ISLR Book Reference by Hasti,Tishirani et al](https://web.stanford.edu/~hastie/Papers/ESLII.pdf)</li> <li> Machine Learning Fundamentals [Andrew Ng's course around ML](https://www.coursera.org/learn/machine-learning) </li> <li> Familiarity with Data Science processes and frameworks [CRISP-DM](https://en.wikipedia.org/wiki/Cross-industry_standard_process_for_data_mining) </li></ul> | Week 0
Handling Text Processing |<ul> <li>Text pre-processing techniques (Familiarity with [spaCy](https://spacy.io/usage) library, familiarity with [NLTK](https://www.nltk.org/) library, [Tokenization using spaCy library](https://medium.com/@makcedward/nlp-pipeline-word-tokenization-part-1-4b2b547e6a3), [Stopword removal and text normalization](https://www.analyticsvidhya.com/blog/2019/08/how-to-remove-stopwords-text-normalization-nltk-spacy-gensim-python/?utm_source=blog&utm_medium=learning-path-nlp-2020) )</li> <li> Regular expressions </li> <li> [Exploratory Analysis](https://towardsdatascience.com/a-complete-exploratory-data-analysis-and-visualization-for-text-data-29fb1b96fb6a) with Text data </li>  <li> [Extract Meta Features from text](https://towardsdatascience.com/understanding-feature-engineering-part-3-traditional-methods-for-text-data-f6f7d70acd41) </li> <li> Build a text classification model [Practice Problem - Identify Sentiments](https://datahack.analyticsvidhya.com/contest/linguipedia-codefest-natural-language-processing-1/?utm_source=blog&utm_medium=learning-path-nlp-2020#LeaderBoard) ..can be any such equivalent problem for experience </li></ul> | Week 1-4


## Techniques

## Libraries / Packages

## Services

## Datasets

## Video and Online Content References

## Research Papers

- [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://openreview.net/forum?id=H1eA7AEtvS), Related [Code](https://github.com/google-research/ALBERT)
- [A Mutual Information Maximization Perspective of Language Representation Learning](https://openreview.net/forum?id=Syx79eBKwr)
- [DeFINE: Deep Factorized Input Token Embeddings for Neural Sequence Modeling](https://openreview.net/forum?id=rJeXS04FPH)


```
End of Content
```


**Disclaimer:** Information represented here is based on my own experiences, learnings, readings and no way represent any firm's opinion, strategy etc or any individual's opinion or not intended for anything else other than learning and/or research/innovation in the field. Content here and on this repository is non-exhaustive and continuous improvement / continuous learning focus is needed to learn more. 
Recommendation - Keep Learning and keep improving.

